<!DOCTYPE html>
<html>
<head>
  <title>Zipf’s Law and Subsets of Lexis</title>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="Zipf_Qualico2018_files/shower-ribbon/package/styles/screen-4x3.css">
  <link rel="stylesheet" href="Zipf_Qualico2018_files/rmdshower/style-common.css">
  <link rel="stylesheet" href="Zipf_Qualico2018_files/rmdshower/style-ribbon.css">
  <link rel="stylesheet" href="Zipf_Qualico2018_files/shower-ribbon/style-override.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
  <script src="Zipf_Qualico2018_files/rmdshower/auto-render.min.js" type="text/javascript"></script>
  
  
    <style type="text/css">
   a.sourceLine { display: inline-block; line-height: 1.25; }
   a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
   a.sourceLine:empty { height: 1.2em; position: absolute; }
   .sourceCode { overflow: visible; }
   code.sourceCode { white-space: pre; position: relative; }
   div.sourceCode { margin: 1em 0; }
   pre.sourceCode { margin: 0; }
   @media screen {
   div.sourceCode { overflow: auto; }
   }
   @media print {
   code.sourceCode { white-space: pre-wrap; }
   a.sourceLine { text-indent: -1em; padding-left: 1em; }
   }
   pre.numberSource a.sourceLine
     { position: relative; }
   pre.numberSource a.sourceLine:empty
     { position: absolute; }
   pre.numberSource a.sourceLine::before
     { content: attr(data-line-number);
       position: absolute; left: -5em; text-align: right; vertical-align: baseline;
       border: none; pointer-events: all;
       -webkit-touch-callout: none; -webkit-user-select: none;
       -khtml-user-select: none; -moz-user-select: none;
       -ms-user-select: none; user-select: none;
       padding: 0 4px; width: 4em;
       color: #aaaaaa;
     }
   pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
   div.sourceCode
     {  }
   @media screen {
   a.sourceLine::before { text-decoration: underline; }
   }
   code span.al { color: #ff0000; font-weight: bold; } /* Alert */
   code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
   code span.at { color: #7d9029; } /* Attribute */
   code span.bn { color: #40a070; } /* BaseN */
   code span.bu { } /* BuiltIn */
   code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
   code span.ch { color: #4070a0; } /* Char */
   code span.cn { color: #880000; } /* Constant */
   code span.co { color: #60a0b0; font-style: italic; } /* Comment */
   code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
   code span.do { color: #ba2121; font-style: italic; } /* Documentation */
   code span.dt { color: #902000; } /* DataType */
   code span.dv { color: #40a070; } /* DecVal */
   code span.er { color: #ff0000; font-weight: bold; } /* Error */
   code span.ex { } /* Extension */
   code span.fl { color: #40a070; } /* Float */
   code span.fu { color: #06287e; } /* Function */
   code span.im { } /* Import */
   code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
   code span.kw { color: #007020; font-weight: bold; } /* Keyword */
   code span.op { color: #666666; } /* Operator */
   code span.ot { color: #007020; } /* Other */
   code span.pp { color: #bc7a00; } /* Preprocessor */
   code span.sc { color: #4070a0; } /* SpecialChar */
   code span.ss { color: #bb6688; } /* SpecialString */
   code span.st { color: #4070a0; } /* String */
   code span.va { color: #19177c; } /* Variable */
   code span.vs { color: #4070a0; } /* VerbatimString */
   code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  
  
  
</head>

<body class="shower list">

  <header class="caption">
    <h1>Zipf’s Law and Subsets of Lexis</h1>
    <p>Maciej Eder, Rafał L. Górski, Joanna Byszuk</p>
  </header>

  
  
<section id="section" class="slide level2 white">
<h2></h2>
<p class="black" style="font-size: 240%; margin-bottom: 0.5em; margin-top: 0.5em;">
Zipf’s Law and Subsets of Lexis
</p>
<p class="black" style="font-size: 120%; margin-bottom: 1em">
Maciej Eder, Rafał L. Górski, Joanna Byszuk
</p>
<p class="black" style="font-size: 80%; margin-bottom: 0.5em;">
Institute of Polish Language (Polish Academy of Sciences)
</p>
<p>
<img src="img/logo_ijp_1.png" width="200px">
</p>
<p class="black" style="font-size: 100%; margin-top: 4em;">
Qualico 2018, Wrocław, 5th July 2018
</p>
</section>
<section id="zipfs-law-and-language" class="slide level2">
<h2>Zipf’s law and language</h2>
<!-- 
* Zauważono, że język jest zipfowski
    * Ale co to znaczy “język”?
    * Czy formy? Czy lematy?
    * Czy to działa w każdym języku?
    * Czy jeśli działa na “Onieginie”, to zadziała dla całego korpusu?
    * Czy zadziała dla poszczególnych kategorii gramatycznych?
* Czy tylko w języku? (por. np. rozkład cząstek chemicznych etc., bogactwa)
Piantadosi (2015) -->
<ul>
<li>It has been observed that the language is Zipfian.</li>
<li>However, what is “the language”?
<ul>
<li>orthographic forms or lemmata?</li>
<li>only words or grammatical categories as well?</li>
<li>unigrams or also n-grams?</li>
</ul></li>
</ul>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p><img data-src="img/qSUgV.jpg" alt="qSUgV" width="700" /></p>
</section>
<section id="zipfs-law-on-a-log-log-scale" class="slide level2">
<h2>Zipf’s Law on a log-log scale</h2>
<p><img data-src="img/jeden_count.png" alt="word_unigrams" width="800" /></p>
</section>
<section id="research-question" class="slide level2">
<h2>Research question</h2>
<ul>
<li>If the distribution of all the words in the corpus is Zipfian, is the distribution of <strong>a subset of these words</strong> also Zipfian?</li>
<li>Since a text is a sum of nouns, verbs, adjectives, prepositions etc., is the distribution of <strong>particular classes</strong> (nouns, verbs etc.) also Zipfian?</li>
</ul>
</section>
<section id="observations-on-brown-corpus" class="slide level2">
<h2>Observations on Brown Corpus</h2>
<blockquote>
<p>[W]ord categories are also fit nicely […] perhaps even more closely than words—but the shape of the fit […] differs.</p>
</blockquote>
<blockquote>
<p>The general pattern suggests that a full explanation of the word frequency distribution would ideally call on mechanisms general enough to apply to syntactic categories and possibly even other levels of analysis.</p>
</blockquote>
<p>(Piantadosi, 2014)</p>
</section>
<section id="dataset" class="slide level2">
<h2>Dataset</h2>
<ul>
<li>The balanced version of the National Corpus of Polish
<ul>
<li>300 mln segments, i.e. roughly 250 mln words.</li>
</ul></li>
<li>The POS tags and lemmata taken as they are.</li>
<li>The corpus tagged automatically, using a 1 mln manually tagged subset.
<ul>
<li>Consequently, some wrongly assigned tags should be expected!</li>
</ul></li>
</ul>
</section>
<section id="categories" class="slide level2">
<h2>Categories</h2>
<ul>
<li>Very granular parts of speech in the National Corpus of Polish.
<ul>
<li>e.g. <em>subst:sg:nom:m1</em>, <em>adj:sg:nom:n:pos</em>, etc.</li>
</ul></li>
<li>For that reason, we split the tags:
<ul>
<li>the proper POS: <em>subst</em>, <em>adj</em>, <em>fin</em>, <em>prep</em>, …</li>
<li>the number: <em>sg</em>, <em>pl</em></li>
<li>the case: <em>nom</em>, <em>gen</em>, <em>dat</em>, …</li>
<li>the person: <em>1st</em>, <em>2nd</em>, <em>3rd</em> (not included in a tag)</li>
</ul></li>
<li>Independently, we tested POS-tag n-grams</li>
</ul>
<!-- * However, there are two “hyper POS-tages”:
  * _Verb_: all traditional verbal classes
  * _Noun_: nouns, gerunds, and pronouns -->
<!-- a POS pcon - is one of 4 Polish participles, a rule of thumb: a Part of Speech is a class, which shares exactly the same set of “Grammatical classes used in the National Corpus of Polish are more precisely delimited and, overall, finer-grained than traditional parts of speech. The classes assumed here are based on the notion of flexeme, narrower than the notion of lexeme. As a rule of thumb: a grammatical class is an entity, which shares a same set of inflecting categories, eg. since present-tense verbs bear person, but not gender and past-tense verbs vice-versa, they are distinct parts of speech-->
</section>
<section id="first-observations-singular-vs.-plural" class="slide level2">
<h2>First observations: <em>singular</em> vs. <em>plural</em></h2>
<p><img data-src="img/sing_plur.png" alt="singular_plural" width="800" /></p>
</section>
<section id="first-observations-1st-2nd-3rd-person" class="slide level2">
<h2>First observations: <em>1st</em>, <em>2nd</em>, <em>3rd person</em></h2>
<p><img data-src="img/1os_2os.png" alt="1st_2nd_person" width="800" /></p>
</section>
<section id="first-observations-cases" class="slide level2">
<h2>First observations: <em>cases</em></h2>
<p><img data-src="img/cases.png" alt="cases" width="800" /></p>
</section>
<section id="first-observations-pos-tag-ngrams" class="slide level2">
<h2>First observations: <em>POS-tag ngrams</em></h2>
<p><img data-src="img/ngrams.png" alt="ngrams" width="800" /></p>
</section>
<section id="modeling-a-power-law-distribution" class="slide level2">
<h2>Modeling a power law distribution</h2>
<ul>
<li>Since linear regression is simple to apply:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">model =<span class="st"> </span><span class="kw">lm</span>( var_frequency <span class="op">~</span><span class="st"> </span>var_rank )</a></code></pre></div>
<ul>
<li>Maybe it could be applied to a log-transformed dataset?</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">model =<span class="st"> </span><span class="kw">lm</span>( <span class="kw">log</span>(var_frequency) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(var_rank) )</a></code></pre></div>
</section>
<section id="linear-regression-on-log-log-data" class="slide level2">
<h2>Linear regression on log-log data</h2>
<blockquote>
<p>If in so doing one discovers a distribution that approximately falls on a straight line, then one can, <strong>if one is feeling particularly bold</strong>, assert that the distribution follows a power law, with a scaling parameter α given by the absolute slope of the straight line.</p>
</blockquote>
<p>(Clauset et al., 2009)</p>
</section>
<section id="fitting-a-power-law" class="slide level2">
<h2>Fitting a power law</h2>
<ul>
<li>Maximum likelihood estimators (MLEs) for continuous datasets</li>
</ul>
<p><span class="math display">\[ \alpha = 1 + n \Big[ \sum_{i=1}^{n} \ln \frac{x_i}{x_{min}} \Big] ^{-1} \]</span></p>
<ul>
<li>MLEs for discrete datasets:</li>
</ul>
<p><span class="math display">\[ \alpha \simeq 1 + n \Big[ \sum_{i=1}^{n} \ln \frac{x_i}{x_{min} - \frac{1}{2}} \Big] ^{-1} \]</span></p>
<ul>
<li><span class="math inline">\(x_{min}\)</span> is estimated using the Kolmogorov-Smirnov (KS) statistic:</li>
</ul>
<p><span class="math display">\[ D = \max_{x \geq x_{min}} | S(x) - P(x) |  \]</span></p>
</section>
<section id="fitted-parameters-x_min-cutoff" class="slide level2">
<h2>Fitted parameters: <span class="math inline">\(x_{min}\)</span> (cutoff)</h2>
<p><img src="Zipf_Qualico2018_files/figure-revealjs/unnamed-chunk-4-1.png" width="816" /></p>
</section>
<section id="fitted-parameters-alpha-scaling" class="slide level2">
<h2>Fitted parameters: <span class="math inline">\(\alpha\)</span> (scaling)</h2>
<p><img src="Zipf_Qualico2018_files/figure-revealjs/unnamed-chunk-5-1.png" width="816" /></p>
</section>
<section id="results" class="slide level2">
<h2>Results</h2>
<ul>
<li>We fitted power law parameters for different categories:
<ul>
<li>grammatical classes (parts of speech)</li>
<li>inflection categories (cases, persons, numbers)</li>
<li>POS-tags combined in 2-grams, 3-grams, …, 8-grams</li>
</ul></li>
<li>We compared <span class="math inline">\(\alpha\)</span> (scaling) of the estimated models</li>
<li>We compared the proportion of observations above <span class="math inline">\(x_{min}\)</span></li>
</ul>
</section>
<section id="what-is-perfectly-zipfian" class="slide level2">
<h2>What is perfectly Zipfian?</h2>
<p>Prepositions and conjuctions:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">POS</th>
<th style="text-align: center;">Occurrences</th>
<th style="text-align: center;"><span class="math inline">\(\alpha\)</span></th>
<th style="text-align: center;">ZTypes</th>
<th style="text-align: center;">ZTokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">prep</td>
<td style="text-align: center;">28,787,398</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">97.9%</td>
<td style="text-align: center;">99.99%</td>
</tr>
<tr class="even">
<td style="text-align: center;">conj</td>
<td style="text-align: center;">10,455,657</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">81.03%</td>
<td style="text-align: center;">99.99%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">comp</td>
<td style="text-align: center;">4,145,149</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">87.23%</td>
<td style="text-align: center;">99.99%</td>
</tr>
</tbody>
</table>
<ul>
<li>Short, closed classes</li>
<li>Non-Zipfian elements include archaic vocabulary, present but underrepresented in relation to the whole corpus.</li>
</ul>
</section>
<section id="what-is-least-zipfian" class="slide level2">
<h2>What is least Zipfian?</h2>
<p>Participles:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">POS</th>
<th style="text-align: center;">Occurrences</th>
<th style="text-align: center;"><span class="math inline">\(\alpha\)</span></th>
<th style="text-align: center;">ZTypes</th>
<th style="text-align: center;">ZTokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">praet</td>
<td style="text-align: center;">11,995,036</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">5.12%</td>
<td style="text-align: center;">81.23%</td>
</tr>
<tr class="even">
<td style="text-align: center;">pant</td>
<td style="text-align: center;">35,235</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">14.54%</td>
<td style="text-align: center;">79.09%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ppas</td>
<td style="text-align: center;">3,187,531</td>
<td style="text-align: center;">2.24</td>
<td style="text-align: center;">4.62%</td>
<td style="text-align: center;">68.1%</td>
</tr>
<tr class="even">
<td style="text-align: center;">pact</td>
<td style="text-align: center;">1,209,948</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">3.5%</td>
<td style="text-align: center;">65.32%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">pcon</td>
<td style="text-align: center;">662,548</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">4.5%</td>
<td style="text-align: center;">64.38%</td>
</tr>
</tbody>
</table>
<ul>
<li>Why? They’re very productive!</li>
</ul>
</section>
<section id="major-parts-of-speech" class="slide level2">
<h2>Major parts of speech</h2>
<ul>
<li>Subject and verb:
<ul>
<li>Tokens - over 99% and 95% respectively</li>
<li>Types - both only 17%</li>
</ul></li>
<li>Adjectives and adverbs:
<ul>
<li>Tokens - 94% and 98%</li>
<li>Types - 10% and 18%</li>
</ul></li>
<li>These are also very productive categories!</li>
</ul>
</section>
<section id="cases" class="slide level2">
<h2>Cases</h2>
<p>Stable, (un)expected Zipf results for both types and tokens:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Case</th>
<th style="text-align: center;">ZTypes</th>
<th style="text-align: center;">ZTokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">acc</td>
<td style="text-align: center;">98%</td>
<td style="text-align: center;">18%</td>
</tr>
<tr class="even">
<td style="text-align: center;">voc</td>
<td style="text-align: center;">95.5%</td>
<td style="text-align: center;">10%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">dat</td>
<td style="text-align: center;">93%</td>
<td style="text-align: center;">7%</td>
</tr>
<tr class="even">
<td style="text-align: center;">loc</td>
<td style="text-align: center;">91%</td>
<td style="text-align: center;">3.5%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">gen</td>
<td style="text-align: center;">88%</td>
<td style="text-align: center;">7%</td>
</tr>
<tr class="even">
<td style="text-align: center;">inst</td>
<td style="text-align: center;">75%</td>
<td style="text-align: center;">2.5%</td>
</tr>
<tr class="odd">
<td style="text-align: center;">nom</td>
<td style="text-align: center;">65%</td>
<td style="text-align: center;">2.5%</td>
</tr>
</tbody>
</table>
</section>
<section id="relation-between-alpha-scaling-and-coverage" class="slide level2">
<h2>Relation between <span class="math inline">\(\alpha\)</span> (scaling) and coverage?</h2>
<ul>
<li>Is there any relation between the parameter <span class="math inline">\(\alpha\)</span> (or slope of the model) and the number of observations above the <span class="math inline">\(x_{min}\)</span> cutoff point?</li>
<li>To address it, we modeled:
<ul>
<li>the relation between <span class="math inline">\(\alpha\)</span> and the proportion of <strong>tokens</strong> <span class="math inline">\(\geq x_{min}\)</span></li>
<li>the relation between <span class="math inline">\(\alpha\)</span> and the proportion of <strong>types</strong> <span class="math inline">\(\geq x_{min}\)</span></li>
</ul></li>
</ul>
</section>
<section id="parameter-alpha-vs.-sqrt-of-zipfian-tokens" class="slide level2">
<h2>Parameter <span class="math inline">\(\alpha\)</span> vs. <span class="math inline">\(\sqrt{\%}\)</span> of Zipfian tokens</h2>
<p><img src="Zipf_Qualico2018_files/figure-revealjs/unnamed-chunk-6-1.png" width="816" /></p>
</section>
<section id="parameter-alpha-vs.-of-zipfian-types" class="slide level2">
<h2>Parameter <span class="math inline">\(\alpha\)</span> vs. <span class="math inline">\(\%\)</span> of Zipfian types</h2>
<p><img src="Zipf_Qualico2018_files/figure-revealjs/unnamed-chunk-7-1.png" width="816" /></p>
</section>
<section id="conclusions" class="slide level2">
<h2>Conclusions</h2>
<ul>
<li>Unzipped language categories do not necessarily follow a power law distribution.</li>
<li>Productivity of a class seems to be responsible for a heavy tail.</li>
<li>Classes of relatively low productivity (<em>voc</em>, <em>praep</em>) do follow Zipf’s law.</li>
<li>The relation between <span class="math inline">\(\alpha\)</span> (the scaling parameter) and the proportion of a class following Zipf’s distribution worth further exploration.</li>
</ul>
</section>
<section id="thank-you" class="slide level2">
<h2>Thank you!</h2>
<ul>
<li><a href="https://twitter.com/MaciejEder">@MaciejEder</a></li>
<li><a href="https://twitter.com/jbyszuk">@jbyszuk</a></li>
<li><a href="https://github.com/computationalstylistics" class="uri">https://github.com/computationalstylistics</a></li>
</ul>
<p>This research is part of project UMO-2013/11/B/HS2/02795, supported by Poland’s National Science Centre.</p>
</section>

  <!--
  To hide progress bar from entire presentation
  just remove “progress” element.
  -->
  <!-- <div class="progress"></div> -->
  <script src="Zipf_Qualico2018_files/rmdshower/node_modules/shower/node_modules/shower-core/shower.min.js"></script>
  <!-- Copyright © 2015 Yours Truly, Famous Inc. -->
  <!-- Photos by John Carey, fiftyfootshadows.net -->

    <script>renderMathInElement(document.body);</script>
  
  
</body>
</html>
